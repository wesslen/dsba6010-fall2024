---
sidebar_position: 10
title: Week 12 - LLMs in Production
---

# LLMs in Production

**Objective**: Learn best practices for deploying LLMs in production environments. Students will understand the challenges and solutions for running LLMs at scale.

## Slides

<iframe src="https://docs.google.com/presentation/d/e/2PACX-1vTJQELXKHG84oRgGbeWpHJSwOVgx2H7A54vxf_MU1CUYIItgpSqDg-GSUQQIgp4bG8RGqQ1DZuL9OEX/embed?start=false&loop=false&delayms=3000" frameborder="0" width="960" height="569" allowfullscreen="true" mozallowfullscreen="true" webkitallowfullscreen="true"></iframe>

## Required Readings
- [The Shift from Models to Compound AI Systems](https://bair.berkeley.edu/blog/2024/02/18/compound-ai-systems/)
- [Optimizing Latency](https://hamel.dev/notes/llm/inference/03_inference.html)
- [A Visual Guide to Quantization](https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization)

## Optional Reading

- [Every Way To Get Structured Output From LLMs](https://www.boundaryml.com/blog/structured-output-from-llms)
- [Berkeley Function-Calling Leaderboard](https://gorilla.cs.berkeley.edu/blogs/8_berkeley_function_calling_leaderboard.html)
