---
sidebar_position: 4
title: Week 5 - LLM Technicals
---

# LLM Fundamentals

**Objective**: Develop a deep understanding of LLM architecture components including tokenization, embeddings, and attention mechanisms. Students will explore the technical foundations that make LLMs work.

## Required Readings
- [Understanding LLM Tokenization](https://christophergs.com/blog/understanding-llm-tokenization)
- [Weird Properties of Large Language Models](https://moreisdifferent.blog/p/weird-properties-of-large-language)
- [Building a Generative AI Platform](https://huyenchip.com/2024/07/25/genai-platform.html)

## Optional Reading

- [Attention in transformers, visually explained | Chapter 6, Deep Learning](https://www.youtube.com/watch?v=eMlx5fFNoYc)
- [The Math behind Attention](https://www.youtube.com/watch?v=UPtG_38Oq8o)
