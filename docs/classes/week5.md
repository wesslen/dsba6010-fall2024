---
sidebar_position: 4
title: Week 5 - LLM Technicals
---

# LLM Fundamentals

**Objective**: Develop a deep understanding of LLM architecture components including tokenization, embeddings, and attention mechanisms. Students will explore the technical foundations that make LLMs work.

## Slides

<iframe src="https://docs.google.com/presentation/d/e/2PACX-1vR8YUIP9dWqyV3ptV-YdLioAKDzpkU8Vq6RSD9LtcH3-SYyD2U_lEMaZ8fqOOxT1kDrLy-8zslO4vXi/embed?start=false&loop=false&delayms=3000" frameborder="0" width="960" height="569" allowfullscreen="true" mozallowfullscreen="true" webkitallowfullscreen="true"></iframe>

## Required Readings
- [Understanding LLM Tokenization](https://christophergs.com/blog/understanding-llm-tokenization)
- [Weird Properties of Large Language Models](https://moreisdifferent.blog/p/weird-properties-of-large-language)
- [Building a Generative AI Platform](https://huyenchip.com/2024/07/25/genai-platform.html)

## Optional Reading

- [Attention in transformers, visually explained | Chapter 6, Deep Learning](https://www.youtube.com/watch?v=eMlx5fFNoYc)
- [The Math behind Attention](https://www.youtube.com/watch?v=UPtG_38Oq8o)
